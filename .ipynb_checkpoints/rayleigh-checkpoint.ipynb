{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, NamedTuple, Tuple, Union\n",
    "Step = int\n",
    "Schedule = Callable[[Step], float]\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import Image, clear_output\n",
    "from PIL import Image\n",
    "import glob, os, shutil\n",
    "import os.path\n",
    "\n",
    "import time\n",
    "\n",
    "import scipy.io as io\n",
    "import scipy.sparse.csgraph as csgraph\n",
    "from scipy.sparse.csgraph import laplacian as csgraph_laplacian\n",
    "import scipy as sp\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.linalg import null_space\n",
    "\n",
    "import jax\n",
    "from jax import jit, vmap, random, grad, value_and_grad, hessian\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental.optimizers import optimizer\n",
    "from jax import numpy as jnp\n",
    "\n",
    "from functools import partial\n",
    "import itertools\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import collections as mc\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from utils import *\n",
    "from optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 2500\n",
      "Number of edges: 4900\n",
      "Average degree:   3.9200\n"
     ]
    }
   ],
   "source": [
    "# load the data from the SuiteSparse Matrix Collection format\n",
    "# https://www.cise.ufl.edu/research/sparse/matrices/\n",
    "graphs = ['qh882','dwt_1005','3elt','commanche_dual','bcsstk31']\n",
    "graphdir = './testcases/'\n",
    "graphpostfix = 'dwt_1005'\n",
    "assert graphpostfix in graphs\n",
    "grid_testcase = nx.grid_graph(dim=(50, 50))\n",
    "#grid_testcase = nx.triangular_lattice_graph(25,25)\n",
    "#grid_testcase = nx.cycle_graph(100)\n",
    "grid_testcase_adjacency = nx.adjacency_matrix(grid_testcase).toarray().astype(np.int16)\n",
    "DEBUG=True\n",
    "if DEBUG:\n",
    "    graph, G, A, L, D, n = load_graph(graphdir+graphpostfix, A=grid_testcase_adjacency, plot_adjacency=False, verbose=True)\n",
    "else:\n",
    "    graph, G, A, L, D, n = load_graph(graphdir+graphpostfix, A=None, plot_adjacency=False, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del G\n",
    "del A\n",
    "del D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def project(X1, C, E_0, c=jnp.array([0,0])):\n",
    "    C1 = X1.T@X1\n",
    "    C1sqrt = utils._sqrtm(C1)\n",
    "    Csqrt = utils._sqrtm(C)\n",
    "    U,s,V = jnp.linalg.svd(Csqrt@C1sqrt)\n",
    "    X = X1@jnp.linalg.inv(C1sqrt)@U@V.T@Csqrt\n",
    "\n",
    "    negdef = jnp.all(jnp.linalg.eigvals(X.T@E_0) <= 1e-8)\n",
    "    U_E, _, V_E = jnp.linalg.svd(X.T@E_0)\n",
    "    X = X@(-U_E@V_E.T)\n",
    "    #X = jax.lax.cond(negdef,\n",
    "    #                 lambda _ : X@(-U_E@V_E.T),\n",
    "    #                 lambda _ : X,\n",
    "    #                 operand=None\n",
    "    #                )\n",
    "    return X.real\n",
    "\n",
    "@jit\n",
    "def _step_noautograd(stp, X_k, A_x, A_y, b_x, b_y):\n",
    "    \"\"\"Perform a single gradient (using autograd) + projection step with adaptive momentum.\"\"\"\n",
    "    E_0 = stp*jnp.vstack([b_x,b_y]).T\n",
    "    X_k_x = X_k[:,0] - stp*A_x@X_k[:,0]\n",
    "    X_k_y = X_k[:,1] - stp*A_y@X_k[:,1]\n",
    "    X_k_t = jnp.vstack([X_k_x,X_k_y]).T - E_0\n",
    "    X_k_t = project(X_k_t, C, E_0)\n",
    "    return X_k_t\n",
    "\n",
    "def pgd(X_k, A_x, A_y, b_x, b_y, C, convergence_criterion, \n",
    "           maxiters=1000, alpha=1e-2, beta=0.9):\n",
    "    \"\"\"Perform iterations of PGD, without autograd.\"\"\"\n",
    "    E_0 = jnp.stack([b_x, b_y], axis=1)\n",
    "    L = jnp.linalg.inv(C)@X_k.T@(A_x@X_k+E_0)\n",
    "    \n",
    "    report = {'x':None, 'lossh':[f(X_k, A_x, A_y, b_x, b_y).item()], 'sln_path':[np.asarray(X_k)], \n",
    "            'foc':[foc_pgd(X_k, L, C, A_x, b_x, b_y).item()], 'step_sizes':[1], 'L':[L]}\n",
    "    \n",
    "    for k in tqdm(range(maxiters)):\n",
    "        # backtracking line search\n",
    "        f_xp = 1e8\n",
    "        stp = 1\n",
    "\n",
    "        f_x, g = value_and_grad(f)(X_k, A_x, A, b_x, b_y)\n",
    "        X_k_t = X_k\n",
    "        derphi=1\n",
    "        while f_xp >= f_x: #- alpha * stp * derphi:\n",
    "            stp *= beta\n",
    "            X_k_t = _step_noautograd(stp, X_k, A_x, A_y, b_x, b_y)    \n",
    "            f_xp = f(X_k_t, A_x, A_y, b_x, b_y)\n",
    "        \n",
    "            if stp < 1e-6:\n",
    "                break  \n",
    "\n",
    "        X_k = X_k_t\n",
    "        L = jnp.linalg.inv(C)@X_k.T@(A_x@X_k+E_0)\n",
    "        \n",
    "        report['lossh'].append(f_xp.item())\n",
    "        report['sln_path'].append(np.asarray(X_k))\n",
    "        report['L'].append(np.asarray(L))\n",
    "        report['step_sizes'].append(stp)\n",
    "        report['foc'].append(foc_pgd(X_k, L, C, A_x, b_x, b_y).item())\n",
    "        \n",
    "        if len(report['lossh']) > 2 and \\\n",
    "        np.abs(report['lossh'][-2] - report['lossh'][-1]) <= convergence_criterion:\n",
    "            print('converged')\n",
    "            break\n",
    "    report['x'] = X_k\n",
    "        \n",
    "    return report\n",
    "\n",
    "@jit\n",
    "def step(i, opt_state, A_x, A_y, b_x, b_y):\n",
    "    \"\"\"Perform a single gradient (using autograd) + projection step with adaptive momentum.\"\"\"\n",
    "    X_k = get_params(opt_state)\n",
    "    f_x, g = value_and_grad(f)(X_k, A_x, A_y, b_x, b_y)\n",
    "    return opt_update(i, g, opt_state), f_x\n",
    "\n",
    "def pgd_autograd(opt_params, A_x, A_y, b_x, b_y, C, convergence_criterion, maxiters=1000):\n",
    "    \"\"\"Perform iterations of PGD, with autograd \"\"\"\n",
    "    opt_state, opt_update, get_params = opt_params\n",
    "    X_k = get_params(opt_state)\n",
    "    E_0 = jnp.stack([b_x, b_y], axis=1)\n",
    "    L = jnp.linalg.inv(C)@X_k.T@(A_x@X_k+E_0)\n",
    "    \n",
    "    report = {'x':None, 'lossh':[f(X_k, A_x, A_y, b_x, b_y).item()], 'sln_path':[np.asarray(X_k)], \n",
    "            'foc':[foc_pgd(X_k, L, C, A_x, b_x, b_y).item()], 'step_sizes':[1], 'L':[L]}\n",
    "    \n",
    "    for k in tqdm(range(maxiters)):\n",
    "        opt_state, f_k = step(k, opt_state, A_x, A_y, b_x, b_y)\n",
    "        X_k = get_params(opt_state)\n",
    "\n",
    "        L = jnp.linalg.inv(C)@X_k.T@(A_x@X_k+E_0)\n",
    "\n",
    "        report['lossh'].append(f_k.item())\n",
    "        report['sln_path'].append(np.asarray(X_k))\n",
    "        report['L'].append(np.asarray(L))\n",
    "        report['foc'].append(foc_pgd(X_k, L, C, A_x, b_x, b_y).item())\n",
    "        \n",
    "        if len(report['lossh']) > 2 and \\\n",
    "        np.abs(report['lossh'][-2] - report['lossh'][-1]) <= convergence_criterion:\n",
    "            print('converged')\n",
    "            break\n",
    "            \n",
    "    report['x'] = X_k\n",
    "        \n",
    "    return report\n",
    "\n",
    "@jit\n",
    "def _step(i, opt_state, Z):\n",
    "    \"\"\"Perform a single descent + projection step with arbitrary descent direction.\"\"\"\n",
    "    return opt_update(i, Z, opt_state)\n",
    "\n",
    "def _D_Z(X, A, P, d, e):\n",
    "    I = jnp.eye(A.shape[0])\n",
    "    #Adinv = jnp.linalg.inv(A + d*I)\n",
    "    #XtADinv = X.T@Adinv\n",
    "    #Del = jnp.linalg.inv(XtADinv@X)@XtADinv@e\n",
    "    #Z = Adinv@(-X@Del + e)\n",
    "    Ad = A + d*I\n",
    "    \n",
    "    Del = jnp.linalg.solve(X.T@jnp.linalg.solve(Ad, X), X.T)@jnp.linalg.solve(Ad, e)\n",
    "    Z = jnp.linalg.solve(Ad, -X@Del + e)\n",
    "    \n",
    "    return Del, Z\n",
    "\n",
    "@jit\n",
    "def _sqp(A, P, L, E_0, X):\n",
    "    \"\"\"Perform an iteration of SQP.\"\"\" \n",
    "    #I = jnp.eye(A.shape[0])\n",
    "    w = jnp.linalg.eigvals(L)\n",
    "    idx = w.argsort()#[::-1]   \n",
    "    w = w[idx]\n",
    "    E = -E_0 - (A@X + X@L)\n",
    "    \n",
    "    Del_0, Z_0 = _D_Z(X, A, P, w[0], E[:,0])\n",
    "    Del_1, Z_1 = _D_Z(X, A, P, w[1], E[:,1])\n",
    "    \n",
    "    Z = jnp.stack([Z_0, Z_1], axis=1)\n",
    "    Del = jnp.stack([Del_0, Del_1], axis=1)\n",
    "    \n",
    "    return Z, Del    \n",
    "\n",
    "def D_Z(X, A, P, d, e):\n",
    "    I = jnp.eye(A.shape[0])\n",
    "    Ad = A + d*I\n",
    "    \n",
    "    ADinvP = jnp.linalg.solve(Ad, P.T)\n",
    "    \n",
    "    Del_1 = (X.T@P)@ADinvP@X\n",
    "    Del = jnp.linalg.solve(Del_1, X.T)@P@ADinvP@e\n",
    "    #Del = jnp.linalg.solve(Del_1, X.T)@jnp.linalg.solve(P@Ad@P.T, e)\n",
    "    \n",
    "    Z = P@ADinvP@(-X@Del + e)\n",
    "    \n",
    "    #Adinv = jnp.linalg.inv(A + d*I)\n",
    "    #XtADinv = ((X.T@P)@Adinv)@P.T\n",
    "    #Del = jnp.linalg.inv(XtADinv@X)@XtADinv@e\n",
    "    #Z = P@(Adinv@(P.T@(-X@Del + e)))\n",
    "\n",
    "    return Del, Z\n",
    "\n",
    "@jit\n",
    "def sqp(A, P, pAp, L, E_0, X):\n",
    "    \"\"\"Perform an iteration of SQP.\"\"\" \n",
    "    #I = jnp.eye(A.shape[0])\n",
    "    w = jnp.linalg.eigvals(L)\n",
    "    idx = w.argsort()#[::-1]   \n",
    "    w = w[idx]\n",
    "    E = -E_0 - (pAp@X + X@L)\n",
    "    \n",
    "    Del_0, Z_0 = D_Z(X, A, P, w[0], E[:,0])\n",
    "    Del_1, Z_1 = D_Z(X, A, P, w[1], E[:,1])\n",
    "    \n",
    "    Z = jnp.stack([Z_0, Z_1], axis=1)\n",
    "    Del = jnp.stack([Del_0, Del_1], axis=1)\n",
    "    \n",
    "    return Z, Del    \n",
    "\n",
    "def newton(opt_params, A, P, L, C, X_k, b_x, b_y, convergence_criterion, \n",
    "           maxiters=100, alpha=1e-2, beta=0.9):\n",
    "    \"\"\"Perform iterations of PND + backtracking line search.\"\"\"    \n",
    "    opt_state, opt_update, get_params = opt_params\n",
    "    X_k = get_params(opt_state)\n",
    "    E_0 = np.stack([b_x, b_y], axis=1)\n",
    "   \n",
    "    pAp = P@A@P.T\n",
    "\n",
    "    L = L_init(X_k, C, pAp, E_0)\n",
    "    #L_sym = (L + L.T)/2\n",
    "    #L = L_sym\n",
    "    \n",
    "    report = {'x':None, 'lossh':[f(X_k, pAp, pAp, b_x, b_y).item()], 'sln_path':[np.asarray(X_k)], \n",
    "            'foc':[foc_sqp(X_k, L, C, pAp, E_0).item()], 'step_sizes':[1], 'L':[L]}\n",
    "    \n",
    "    cc = 0\n",
    "\n",
    "    for k in tqdm(range(maxiters)):         \n",
    "        #Z, Del = sqp(A, P, pAp, L, E_0, X_k)\n",
    "        Z, Del = _sqp(pAp, P, L, E_0, X_k)\n",
    "        \n",
    "        # backtracking line search\n",
    "        f_xp = 1e8\n",
    "        stp = 1\n",
    "        f_x, gr = value_and_grad(f)(X_k, pAp, pAp, b_x, b_y)\n",
    "        #derphi = jnp.trace(gr.T@Z)\n",
    "        derphi = 1\n",
    "        len_p = jnp.linalg.norm(Z)\n",
    "        X_k_t = X_k\n",
    "        \n",
    "        opt_state_t = opt_state\n",
    "        \n",
    "        while f_xp >= f_x: #- alpha * stp * derphi:\n",
    "            stp *= beta\n",
    "            opt_state_t = _step(stp, opt_state, -Z)\n",
    "            X_k_t = get_params(opt_state_t)\n",
    "            f_xp = f(X_k_t, pAp, pAp, b_x, b_y)\n",
    "        \n",
    "            if stp * len_p < 1e-8:\n",
    "                break    \n",
    "        L = L + stp*Del\n",
    "        foc = foc_sqp(X_k, L, C, pAp, E_0)\n",
    "        \n",
    "        opt_state = opt_state_t\n",
    "        X_k = get_params(opt_state_t)\n",
    "        \n",
    "        report['sln_path'].append(np.asarray(X_k))\n",
    "        report['step_sizes'].append(stp)\n",
    "        report['foc'].append(foc.item())\n",
    "        report['lossh'].append(f_xp.item())\n",
    "        report['L'].append(np.asarray(L))\n",
    "        \n",
    "        if len(report['lossh']) > 2 and np.abs(report['lossh'][-2] - report['lossh'][-1]) <= convergence_criterion:\n",
    "            cc += 1\n",
    "            if cc > 10:\n",
    "                print('converged')\n",
    "                break\n",
    "        if cc > 0:\n",
    "            cc -= 1\n",
    "            \n",
    "    return report\n",
    "    \n",
    "def ssm(opt_params, A, P, L, C, X_k, b_x, b_y, convergence_criterion, \n",
    "        maxiters=10, alpha=1e-2, beta=0.9):\n",
    "    \"\"\"\n",
    "    1. compute newton direction z = sqp(X, Z, v, Ax + E0) & subspace S\n",
    "    2. approximate locally optimal X, L on S; X = min F(\\hat{X}, B, V.T@E0)\n",
    "    \"\"\"\n",
    "    opt_state, opt_init, opt_update, get_params = opt_params\n",
    "    X_k = get_params(opt_state)\n",
    "    E_0 = np.stack([b_x, b_y], axis=1)\n",
    "   \n",
    "    pAp = P@A@P.T\n",
    "\n",
    "    #L = L_init(X_k, C, pAp, E_0)\n",
    "    \n",
    "    #report = {'x':None, 'lossh':[f(X_k, pAp, pAp, b_x, b_y).item()], 'sln_path':[np.asarray(X_k)], \n",
    "    #        'foc':[foc_sqp(X_k, L, C, pAp, E_0).item()], 'step_sizes':[1], 'L':[L]}\n",
    "    \n",
    "    cc = 0\n",
    "    v = np.zeros_like(X_k)\n",
    "    \n",
    "    results = None\n",
    "    \n",
    "    for k in tqdm(range(maxiters)):     \n",
    "        \n",
    "        'Subspace computation'\n",
    "        Z, Del = _sqp(pAp, P, L, E_0, X_k)\n",
    "        S = [X_k, Z, v, pAp@X_k+E_0]        \n",
    "        Q, _ = jnp.linalg.qr(jnp.concatenate(S,axis=-1), mode='reduced')\n",
    "        B=Q.T@pAp@Q\n",
    "        \n",
    "        'initialize wrt subspace'\n",
    "        X_k = np.linalg.pinv(Q)@X_k\n",
    "        E_0 = Q.T@E_0\n",
    "        X_k = project(X_k, C, E_0, np.zeros(2))\n",
    "        \n",
    "        #L = np.eye(2)\n",
    "        L = L_init(X_k, C, B, E_0)\n",
    "        _, opt_init, opt_update, get_params = opt_params\n",
    "\n",
    "        opt_state = opt_init(X_k)       \n",
    "        # rly need to fix optimizer initialization.\n",
    "        result = newton((opt_state, opt_update, get_params), pAp, Q.T, L, C, X_k, E_0[:,0], E_0[:,1], \n",
    "                        convergence_criterion=convergence_criterion, maxiters=100, alpha=0.0, beta=0.9) \n",
    "        \n",
    "        w_v, v_v = jnp.linalg.eig(B)\n",
    "        idx = w_v.argsort()\n",
    "        v_v = v_v[idx]\n",
    "        v=Q@v_v[:,1:3]\n",
    "        X_k = result['sln_path'][-1]\n",
    "        L = result['L'][-1]\n",
    "        \n",
    "        X_k = Q@X_k\n",
    "        E_0 = Q@E_0\n",
    "        if results == None:\n",
    "            results = result\n",
    "            results['sln_path'] = [X_k]\n",
    "            results['lossh'] = [result['lossh'][-1]]            \n",
    "        \n",
    "        results['lossh'].extend(result['lossh'])\n",
    "        results['sln_path'].extend([X_k]*len(result['lossh']))\n",
    "        results['foc'].extend(result['foc'])\n",
    "        results['step_sizes'].extend(result['step_sizes'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def map_vars(A, X_k, fixed_indices, centercons, decomp=True):\n",
    "    \"\"\"Preprocess variables \"\"\" \n",
    "    N = A.shape[0] \n",
    "    k = fixed_indices.shape[0]\n",
    "    fixed_idx = np.zeros((k,N))\n",
    "    for i in range(k):\n",
    "        fixed_idx[i,fixed_indices[i]] += 1\n",
    "        \n",
    "    if decomp:\n",
    "        boolean_fixed_idx = fixed_idx.sum(0).astype(bool)\n",
    "        boolean_nonfixed_idx = (1-fixed_idx.sum(0)).astype(bool)\n",
    "\n",
    "        X_1x = X_k[boolean_fixed_idx,0]\n",
    "        X_1y = X_k[boolean_fixed_idx,1]\n",
    "        X_2  = X_k[boolean_nonfixed_idx]\n",
    "        A_12 = A[boolean_fixed_idx, :]\n",
    "        A_12 = A_12[:, boolean_nonfixed_idx] \n",
    "\n",
    "        b_x = X_1x@A_12\n",
    "        b_y = X_1y@A_12\n",
    "        X_k = X_2\n",
    "\n",
    "        A = A[boolean_nonfixed_idx, :]\n",
    "        A = A[:,boolean_nonfixed_idx]\n",
    "        P = np.eye(A.shape[0])\n",
    "\n",
    "        constraints = np.expand_dims(np.ones(X_2.shape[0]),0)\n",
    "        P = null_space(constraints).T\n",
    "        P = P * np.sign(P[0,0])\n",
    "        pinvcons = np.linalg.pinv(constraints)\n",
    "        n0_x = pinvcons@(np.expand_dims(centercons[0],0))\n",
    "\n",
    "        b_x = b_x@P.T \n",
    "        if centercons[1] == centercons[0]:\n",
    "            n0_y = n0_x\n",
    "        else:\n",
    "            n0_y = pinvcons@(np.expand_dims(centercons[1],0))\n",
    "        b_y = b_y@P.T \n",
    "        \n",
    "        ### do this computation later to keep A's sparsity\n",
    "        #A = P@A@P.T\n",
    "    else:\n",
    "        if k>0:\n",
    "            fixed_coordsx = X_k[fixed_indices,0]\n",
    "            fixed_coordsy = X_k[fixed_indices,1]\n",
    "\n",
    "            constraints = np.concatenate([fixed_idx,np.expand_dims(1-fixed_idx.sum(0),0)])\n",
    "            fixed_coordsx = np.concatenate([fixed_coordsx,np.expand_dims(centercons[0],0)])\n",
    "            fixed_coordsy = np.concatenate([fixed_coordsy,np.expand_dims(centercons[1],0)])\n",
    "            P = null_space(constraints).T\n",
    "            #_,P = qr_null(constraints).T\n",
    "\n",
    "            pinvcons = np.linalg.pinv(constraints)\n",
    "\n",
    "            n0_x = (pinvcons@fixed_coordsx)\n",
    "            b_x = (P@(A@n0_x))\n",
    "\n",
    "            n0_y = (pinvcons@fixed_coordsy)\n",
    "            b_y = (P@(A@n0_y))\n",
    "            A = (P@A@P.T)\n",
    "        else:\n",
    "            constraints = np.expand_dims(np.ones(n),0)    \n",
    "            P = null_space(constraints).T\n",
    "            #_,P = qr_null(constraints).T\n",
    "\n",
    "            pinvcons = np.linalg.pinv(constraints)\n",
    "            n0_x = pinvcons@(np.expand_dims(centercons[0],0))\n",
    "            b_x = P@(A@n0_x)\n",
    "\n",
    "            n0_y = pinvcons@(np.expand_dims(centercons[1],0))\n",
    "            b_y = P@(A@n0_y)\n",
    "\n",
    "            A = P@A@P.T \n",
    "        \n",
    "    return X_k, A, P, b_x, b_y, n0_x, n0_y, fixed_idx\n",
    "    \n",
    "\n",
    "def cluster(rng, opt_params, X_k, fixed_x, A, mapped_vars, fixed_indices=None, maxiters=1000, convergence_criterion=1e-3,\n",
    "            c1=1, c2=1, c3=0, centroid=jnp.array([0,0]), centercons=None, v=None, D=None, eps=1e-8, method='pgd'):\n",
    "    \"\"\"Given an adjacency matrix A and initialization X_k, optimize X.\"\"\"\n",
    "    method = method.lower()\n",
    "    global opt_update\n",
    "    #opt_init, opt_update, get_params = opt_params    \n",
    "    \n",
    "    assert method in ['pgd','pnd','ssm']\n",
    "    assert len(A.shape) == 2\n",
    "    #assert A.shape[0] == X_k.shape[0]\n",
    "    \n",
    "    k = fixed_x.shape[0]\n",
    "    if fixed_indices is None:\n",
    "        fixed_coordsx = fixed_x[:,0]\n",
    "        fixed_coordsy = fixed_x[:,1]\n",
    "    else:\n",
    "        fixed_coordsx = X_k[fixed_indices,0]\n",
    "        fixed_coordsy = X_k[fixed_indices,1]\n",
    "    \n",
    "    N = A.shape[0]\n",
    "\n",
    "    if v is None:\n",
    "        v = jnp.ones(N)\n",
    "    if D is None:\n",
    "        D = jnp.diag(v)\n",
    "    if centercons is None:\n",
    "        centercons = jnp.zeros(2)\n",
    "    A, P, b_x, b_y, n0_x, n0_y, fixed_idx = mapped_vars\n",
    "\n",
    "    C = jnp.block([[c1, c3],[c3, c2]])\n",
    "    assert jnp.linalg.det(C) > 1e-5\n",
    "    \n",
    "    E_0 = jnp.stack([b_x, b_y], axis=1)\n",
    "    n0 = jnp.stack([n0_x,n0_y],axis=0)\n",
    "    X_k_n = jnp.array(np.linalg.pinv(P.T)@(X_k-n0.T))\n",
    "    X_k_n = project(X_k_n, C, E_0, centercons)\n",
    "    L = np.eye(2)\n",
    "\n",
    "    assert not method == 'pgd', \"delaying PAP decomposition. Use newtons method/pnd\"\n",
    "    if method == \"pgd\":\n",
    "        opt_init, opt_update, get_params = opt_params\n",
    "        opt_state = opt_init(X_k_n)\n",
    "        A_x = A\n",
    "        A_y = A\n",
    "\n",
    "        #result = pgd_autograd((opt_state, opt_update, get_params), A_x, A_y, b_x, b_y, C, \n",
    "        #                      convergence_criterion=convergence_criterion, maxiters=maxiters)\n",
    "        \n",
    "        result = pgd(X_k_n, A_x, A_y, b_x, b_y, C, \n",
    "                        convergence_criterion=convergence_criterion, maxiters=maxiters, alpha=0.5, beta=0.9)  \n",
    "    elif method == \"pnd\":\n",
    "        opt_init, opt_update, get_params = opt_params\n",
    "        opt_state = opt_init(X_k_n)\n",
    "        result = newton((opt_state, opt_update, get_params), A, P, L, C, X_k_n, b_x, b_y, \n",
    "                        convergence_criterion=convergence_criterion, maxiters=maxiters, alpha=0.0, beta=0.9)        \n",
    "    elif method =='ssm':\n",
    "        opt_init, opt_update, get_params = opt_params\n",
    "        opt_state = opt_init(X_k_n)\n",
    "        result = ssm((opt_state, opt_init, opt_update, get_params), A, P, L, C, X_k_n, b_x, b_y, \n",
    "                        convergence_criterion=convergence_criterion, maxiters=10, alpha=0.0, beta=0.9)        \n",
    "    else:\n",
    "        print(\"method not supported\")\n",
    "        assert False\n",
    "    X_k = result['sln_path'][np.argmin(result['lossh'])]\n",
    "\n",
    "    X_k = project(X_k, C, E_0, centercons)\n",
    "    X_k_n = np.zeros((N,2))\n",
    "    if fixed_indices is None:\n",
    "        X_k_n[:,0] = np.concatenate([fixed_coordsx, np.array(P.T@X_k[:,0]) + n0_x.T])\n",
    "        X_k_n[:,1] = np.concatenate([fixed_coordsy, np.array(P.T@X_k[:,1]) + n0_y.T])\n",
    "    else:\n",
    "        X_k_n[:,0] = np.array(P.T@X_k[:,0]) + n0_x.T\n",
    "        X_k_n[:,1] = np.array(P.T@X_k[:,1]) + n0_y.T\n",
    "        \n",
    "    result['x'] = X_k_n\n",
    "    \n",
    "    mask = (1-fixed_idx.sum(0)).astype(np.bool)\n",
    "    result['mask'] = mask\n",
    "    result['centroid'] = centercons\n",
    "    if fixed_idx.sum() == 0:\n",
    "        result['g'] = np.array(g(X_k_n, v, centercons))\n",
    "        result['h'] = np.array(h(X_k_n, np.diag(v), c1, c2, c3, centroid))      \n",
    "    else:\n",
    "        # need to insert at mask, not cat/stack\n",
    "        result['g'] = np.array(g(X_k_n[mask], v[mask], centercons))\n",
    "        result['h'] = np.array(h(X_k_n[mask], np.diag(v[mask]), c1, c2, c3, centroid))\n",
    "    result['P'] = (P)\n",
    "    result['e'] = np.vstack([b_x,b_y])\n",
    "    result['n'] = (n0_x, n0_y)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### USER PARAMETERS #####\n",
    "method = \"ssm\" # pnd, ssm, or pgd\n",
    "\n",
    "\n",
    "seed = 0 # random seed\n",
    "eps = 1e-8 # global epsilon variable\n",
    "rng = random.PRNGKey(seed)\n",
    "key, subkey = jax.random.split(rng)\n",
    "\n",
    "v = np.ones(n)\n",
    "c1=v.sum()*10**2*1/12\n",
    "c2=v.sum()*10**2*1/12\n",
    "c3=0\n",
    "C = jnp.block([[c1, c3],[c3, c2]])\n",
    "\n",
    "X_k_r = (random.normal(subkey, (n,2))*np.sqrt(10))\n",
    "\n",
    "if os.path.isfile(graphdir+graphpostfix+'_evals.npy') and \\\n",
    "   os.path.isfile(graphdir+graphpostfix+'_evecs.npy'):\n",
    "    w = np.load(graphdir+graphpostfix+'_evals.npy')\n",
    "    v = np.load(graphdir+graphpostfix+'_evecs.npy')    \n",
    "else:\n",
    "    w,v = sp.sparse.linalg.eigsh(L, k=min(n,5), which='SM')\n",
    "    np.save(graphdir+graphpostfix+'_evals.npy',w)\n",
    "    np.save(graphdir+graphpostfix+'_evecs.npy',v)\n",
    "if DEBUG:\n",
    "    w,v = sp.sparse.linalg.eigsh(L, k=min(n,5), which='SM')\n",
    "X_k = v[:,1:3].real\n",
    "\n",
    "if DEBUG:\n",
    "    fixed_indices = np.array([0])\n",
    "else:\n",
    "    fixed_indices = np.array([0,1,2,3,4,5,6,7,8,9,10])\n",
    "\n",
    "\n",
    "X_k[fixed_indices] = X_k_r[fixed_indices]\n",
    "\n",
    "#X_k = X_k.astype(jnp.float16)\n",
    "#L = L.astype(jnp.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del w\n",
    "del v\n",
    "#del X_k_r\n",
    "v = jnp.ones(n)\n",
    "X_k, A, P, b_x, b_y, n0_x, n0_y, fixed_idx = map_vars(L, X_k, fixed_indices, v.sum()*jnp.array([0,0]))\n",
    "mapped_vars = (A, P, b_x, b_y, n0_x, n0_y, fixed_idx)\n",
    "\n",
    "if method == \"pgd\":\n",
    "    pgd_lr = 5e-2\n",
    "    opt_init, opt_update, get_params = padam(pgd_lr,partial(lambda x, y, z: project(z, y, x), \n",
    "                                                    np.stack([b_x,b_y],axis=1), C), b1=0.9, b2=0.999, eps=1e-08)\n",
    "elif method == \"pnd\":\n",
    "    opt_init, opt_update, get_params = psgd(partial(lambda x, y, z: project(z, y, x), \n",
    "                                                    np.stack([b_x,b_y],axis=1), C))\n",
    "elif method == 'ssm':\n",
    "    #opt_init, opt_update, get_params = psgd(partial(lambda x, y, z: project(z, y, x), \n",
    "    #                                                np.zeros((8,2)), C))    \n",
    "    opt_init, opt_update, get_params = psgd(partial(lambda x, y, z: z, \n",
    "                                                    np.zeros((8,2)), C))    \n",
    "else:\n",
    "    print('method not supported')\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1196de9bdbb6436c947556e6bd12aedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = cluster(rng, (opt_init, opt_update, get_params), \n",
    "                 X_k, X_k_r[fixed_indices], L, mapped_vars, fixed_indices=None, c1=c1, c2=c2, c3=c3, centercons=v.sum()*jnp.array([0,0]), \n",
    "                 v=None, D=None, eps=1e-8, maxiters=100, convergence_criterion=5e-4, method=method)\n",
    "results = [result]\n",
    "X_k_n=result['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eig(result['L'][-1].real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(np.linalg.eig(P@A@P.T)[0])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = utils.plot_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_id, voxel_bound = voxel_cluster(X_k, np.array([5, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.plot_graph(X_k, graph, title='loss: {} h: {} g: {} foc: {}'.format(str(np.round(np.min(result['lossh']),2)), \n",
    "#                                                                            np.round(result['g'],2), np.round(result['g'],2), \n",
    "#                                                                           str(np.round(result['foc'][np.argmin(result['lossh'])],2))), fixed_indices=fixed_indices, c=voxel_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.plot_graph(X_k_n, graph, title='loss: {} h: {} g: {} foc: {}'.format(str(np.round(np.min(result['lossh']),2)), \n",
    "                                                                            np.round(result['h'],2), np.round(result['g'],2), \n",
    "                                                                           str(np.round(result['foc'][np.argmin(result['lossh'])],2))), fixed_indices=fixed_indices, c=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_animation(results, graph, fixed_coordinates=X_k_r[fixed_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
